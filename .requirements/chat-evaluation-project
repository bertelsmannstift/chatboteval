**1.Project scope and workflow []{.mark}**

[]{.mark}

**[Substantive Focus]{.mark}**

[This project develops an evaluation framework for retrieval-augmented
generation (RAG) chatbots. The framework is designed to be generally
applicable to RAG-based systems, but its development and testing in this
pilot are anchored in a concrete use case.]{.mark}

[The substantive focus of the pilot is *PublikationsBot*, an internal
chatbot operated by the Bertelsmann Stiftung. PublikationsBot is
intended to support internal research and knowledge work by enabling
users to search, compare, and synthesize content across a large corpus
of publications.]{.mark}

[]{.mark}

**[Core Deliverables]{.mark}**

A\) Prototype Evaluation Framework

-   Reproducible evaluation pipeline.

-   One-command execution (with options), runnable locally and in CI

-   Runs end-to-end on a small pilot dataset

-   Ingests query--response pairs, optionally with reference documents
    and metadata

-   Produces a single results bundle (tables, plots, summary outputs)

-   Including diagnostics for correctness, grounding, and relevance

> []{.mark}

B\) Annotation Interface to Label Query--Response Pairs

-   Internal-facing web-based annotation interface

-   Uses a predefined set of evaluation criteria

-   Optionally supports:

    a.  Reference answers

    b.  Free-text notes and evidence pointers

    c.  Reviewer metadata and flags

-   Reviewers complete batches; results are exportable as a clean CSV
    (or equivalent)

C\) Report and Best-Practice Brief

-   Documentation of the evaluation framework and annotation interface

-   Findings report summarizing results from the pilot

-   Best-practice brief with recommendations and next steps

**[Workflow and Logistics]{.mark}**

A\) Communication and Coordination

-   Weekly short sync meeting (max. 30 minutes)

    a.  Focus on open questions, upcoming tasks, and weekly deadlines

    b.  Scheduled for Monday mornings

-   Ongoing communication via a dedicated Teams / Slack channel

    a.  Used for quick questions, clarifications, and coordination
        between meetings

B\) Task Planning and Ownership

-   Planned work is curated as small, well-scoped GitHub Issues

    a.  Includes a brief specification of the task (Goal, Scope,
        Implementation requirements, Testing requirements)

    b.  Issues are labeled as "Task" type and assigned to a responsible
        person

-   Issues serve as the primary coordination artifact

    a.  Open questions and design decisions should be documented in the
        Issue discussion when possible

B\) Development Workflow (GitHub)[]{.mark}

-   Development follows the internal contributing guidelines

-   Work is carried out on Issue-specific branches

-   Changes are proposed via Pull Requests (no pushes to main)

    a.  Pull Requests are initially opened as draft

    b.  Once ready, Pull Requests are marked as ready for review and
        assigned to one reviewer

-   Review feedback is addressed iteratively until approval

[]{.mark}

[]{.mark}

**2. Design Details and Technical Approach[]{.mark}**

[]{.mark}

**[Evaluation Framework]{.mark}**

-   Implemented as a Python package within *chatboteval* repository

-   [Monorepo layout integrating the framework and the interface
    under:]{.mark}

    a.  [src/chatboteval]{.mark}

    b.  [apps/annotation]{.mark}

-   Exposed via both Python API and CLI

-   Designed to be generic and extensible, without pilot-specific
    assumptions

-   CI for code quality and reproducibility

-   Optional CD extensions at later stages

-   [Two modes: inference vs. training, whereby inference requires a
    stored trained model (produced by training mode)]{.mark}

[]{.mark}

**[Annotation Interface]{.mark}**

A\) Prompt and Response Generation[]{.mark}

-   Construct a prompt set that approximates real chatbot usage:

    a.  Domain-typical questions aligned with the chatbot's intended use
        cases and content scope

    b.  Ambiguous or challenging cases

-   Systematic prompt variation along:

    a.  Multilingual versions and paraphrases

    b.  Fairness probes (e.g. gender-coded or demographic-coded
        phrasing)

-   AI-assisted prompt generation and augmentation (one-off)

    a.  [Ideally from structured, non-sensitive domain inputs]{.mark}

-   Model outputs should ideally include both responses and retrieved
    context documents

B\) Annotation Schema[]{.mark}

-   [For labeling query-response pairs generated in A) and tailored for
    generation of training and evaluation-relevant data for evaluating
    chatbot response]{.mark} correctness, retrieval grounding, and
    retrieval relevance.[]{.mark}

-   Define a structured, multilabel annotation schema covering:

    a.  Correctness

    b.  Grounding

    c.  Relevance

    d.  Harmfulness / bias flags

    e.  Additional criteria as needed[]{.mark}

C\) Annotation Interface[]{.mark}

-   Implemented within the *chatboteval* repository

    a.  under apps/annotation (or apps/label_ui or apps/labelstudio)

    b.  [either via pure python app using streamlit or using somethin
        like Label Studio]{.mark}

-   Two usage modes:

    a.  Local (CLI / API)

        i.  Run via a command such as chatboteval label_pairs ...

        ii. Local web interface opens for annotation

    ```{=html}
    <!-- -->
    ```
    a.  Hosted (non-technical reviewers)

        i.  Deployed backend accessible via a simple URL

        ii. Admin uploads datasets

        iii. Reviewers access the interface and complete annotation
             tasks

-   Storage abstraction:

    a.  Local mode: SQLite

    b.  Hosted mode: PostgreSQL (or equivalent)

    c.  Canonical export format (CSV / Parquet)

-   Reviewers see prompt, response, and evaluation criteria

-   Labels and comments are recorded and exported as a structured
    dataset[]{.mark}

[]{.mark}