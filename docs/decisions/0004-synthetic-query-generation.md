# 0004 Synthetic Query Generation

Status: Accepted


## Decision

This project enables the creation of chatbot queries when real user queries cannot be collected. 

The following architectural constraints apply:

- **Synthetic query generation**
  - Queries are generated by an LLM-based workflow
- **Promptless, spec-driven interface**
  - Users provide a small structured input spec (constraints + knobs), not prompts.
  - Prompt templates are internal implementation details.
- **Decouple inputs from outputs via sampling**
  - The number of generated queries is controlled explicitly not tied 1:1 to the number of provided seed items
  - Output diversity is enforced deterministically via sampling/selection constraints
- **Query generation is staged and schema-driven**
  - Structured candidate specifications are produced and then realized into query text.
  - Deterministic selection enforces volume/diversity constraints
- **LLM orchestration framework**
  - LangChain is used for prompt templating, LLM invocation, and schema-constrained structured outputs at LLM boundaries.
  - The workflow depends on LangChainâ€™s chat model interface rather than a single vendor SDK, enabling configurable backends (local, Hugging Face, hosted APIs).
  - Out of scope: Graph-based / agent-style orchestration (e.g., LangGraph)

## Rationale

- **Synthetic query generation enables dataset construction under data constraints**
  - In many real-world deployments, real user queries cannot be collected due to data protection, contractual, or operational constraints.
- **Promptless, spec-driven interface ensures clarity and separation of concerns**
  - Reduces cognitive overhead, prevents prompt engineering from becoming part of UX, and separates user intent from internal mechanics.
- **Decoupling inputs from outputs enables controlled scaling and diversity**
  - Treating query generation as a sampling problem allows independent control of output volume and systematic enforcement of coverage and diversity constraints.
- **Staged, schema-driven generation enables enforceable control over generation**
  - Generating free-form text directly makes volume and diversity constraints unenforceable; introducing a structured intermediate representation ensures that candidate queries can be validated, counted, and selected deterministically before surface realization.
- **A framework-level LLM abstraction enforces clean boundaries and flexibility**
  - Using a framework-level LLM abstraction isolates prompt and schema handling at clear boundaries while avoiding lock-in to a single provider and keeping core workflow logic independent of vendor-specific SDKs.



## Consequences

- **A user-facing query generation spec becomes a stable contract**
  - The project must define and maintain a documented input spec for query generation.
- **A structured intermediate representation is required**
  - The project must define schemas for candidate specifications
- **Model/provider configuration becomes a supported surface**
  - The workflow must accept a configurable LangChain chat model backend and document supported configuration patterns.
- **No graph/agent orchestration is supported** 
  - The workflow is implemented as a straight-line pipeline; introducing graph/agent orchestration would require a major overhaul.
